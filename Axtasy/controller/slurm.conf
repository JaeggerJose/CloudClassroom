# slurm.conf file generated by configurator.html.
# Put this file on all nodes of your cluster.
# See the slurm.conf man page for more information.
#
SlurmctldHost=a100
SlurmctldAddr=10.0.3.252
#
#DisableRootJobs=NO
#EnforcePartLimits=NO
#Epilog=
#EpilogSlurmctld=
#FirstJobId=1
#MaxJobId=999999
#GresTypes=
#GroupUpdateForce=0
#GroupUpdateTime=600
#JobFileAppend=0
#JobRequeue=1
#JobSubmitPlugins=1
#KillOnBadExit=0
#LaunchType=launch/slurm
#Licenses=foo*4,bar
MailProg=/opt/slurm-mail/mail_script.sh
#MaxJobCount=5000
#MaxStepCount=40000
#MaxTasksPerNode=128
MpiDefault=none
#MpiParams=ports=#-#
#PluginDir=
#PlugStackConfig=
#PrivateData=jobs
ProctrackType=proctrack/cgroup
#Prolog=
PrologFlags=x11
#PrologSlurmctld=
#PropagatePrioProcess=0
#PropagateResourceLimits=
#PropagateResourceLimitsExcept=
#RebootProgram=
ReturnToService=1
#SallocDefaultCommand=
SlurmctldPidFile=/run/slurmctld.pid
SlurmctldPort=6817
SlurmdPidFile=/run/slurmd.pid
SlurmdPort=6818
SlurmdSpoolDir=/var/lib/slurm-llnl/slurmd
SlurmUser=slurm
#SlurmdUser=root
#SrunEpilog=
#SrunProlog=
StateSaveLocation=/var/lib/slurm-llnl/slurmctld
SwitchType=switch/none
#TaskEpilog=
TaskPlugin=task/affinity,task/cgroup
TaskPluginParam=Sched
#TaskProlog=
#TopologyPlugin=topology/tree
#TmpFS=/tmp
#TrackWCKey=no
#TreeWidth=
#UnkillableStepProgram=
#UsePAM=0
#
#
# TIMERS
#BatchStartTimeout=10
#CompleteWait=0
#EpilogMsgTime=2000
#GetEnvTimeout=2
#HealthCheckInterval=0
#HealthCheckProgram=
InactiveLimit=0
KillWait=30
#MessageTimeout=10
#ResvOverRun=0
MinJobAge=300
#OverTimeLimit=0
SlurmctldTimeout=120
SlurmdTimeout=300
#UnkillableStepTimeout=60
#VSizeFactor=0
Waittime=0
#
#
# SCHEDULING
#DefMemPerCPU=0
FastSchedule=1
#MaxMemPerCPU=0
#SchedulerTimeSlice=30
SchedulerType=sched/backfill
SelectType=select/cons_tres
SelectTypeParameters=CR_Core_Memory
#
#
# JOB PRIORITY
#PriorityFlags=
PriorityType=priority/multifactor
# This determines the contribution of historical usage on the composite usage value. The larger the number, the longer past usage affects fair-share. If set to 0 no decay will be applied. This is helpful if you want to enforce hard time limits per association. If set to 0 PriorityUsageResetPeriod must be set to some interval. The unit is a time string (i.e. min, hr:min:00, days-hr:min:00, or days-hr). The default value is 7-0 (7 days).
PriorityDecayHalfLife=7-0
# The period of time in minutes in which the half-life decay will be re-calculated. The default value is 5 (minutes).
PriorityCalcPeriod=5
# A boolean that sets the polarity of the job size factor. The default setting is NO which results in larger node sizes having a larger job size factor. Setting this parameter to YES means that the smaller the job, the greater the job size factor will be.
PriorityFavorSmall=NO
# Specifies the queue wait time at which the age factor maxes out. The unit is a time string (i.e. min, hr:min:00, days-hr:min:00, or days-hr). The default value is 7-0 (7 days).
PriorityMaxAge=7-0
PriorityUsageResetPeriod=NOW
# An unsigned integer that scales the contribution of the age factor.
PriorityWeightAge=1000
PriorityWeightFairshare=1500
PriorityWeightJobSize=0
PriorityWeightPartition=1500
PriorityWeightQOS=1000
#PriorityWeightTRES=CPU=1000,Mem=1000,gres/gpu=3000,gres/gpu:7g.40gb=3000,gres/gpu:1g.5gb=3000,gres/gpu:2g.10gb=3000,gres/gpu:3g.20gb=3000

# LOGGING AND ACCOUNTING
AccountingStorageEnforce=associations
#AccountingStorageHost=localhost
#AccountingStorageLoc=slurm_acct_db
#AccountingStoragePass=slurmdbpass
#AccountingStoragePort=3306
AccountingStorageType=accounting_storage/slurmdbd
AccountingStorageTRES=gres/gpu,gres/gpu:7g.40gb,gres/gpu:1g.5gb,gres/gpu:2g.10gb,gres/gpu:3g.20gb

#AccountingStorageUser=slurm
AccountingStoreJobComment=YES
ClusterName=dynomics
#DebugFlags=
#JobCompHost=localhost
#JobCompLoc=slurm_acct_db
#JobCompPass=slurmdbpass
#JobCompPort=3306
#JobCompType=jobcomp/slurmdbd
#JobCompUser=slurm
#JobContainerType=job_container/none
JobAcctGatherFrequency=30
JobAcctGatherType=jobacct_gather/none
SlurmctldDebug=info
SlurmctldLogFile=/var/log/slurm-llnl/slurmctld.log
SlurmdDebug=info
SlurmdLogFile=/var/log/slurm-llnl/slurmd.log
#SlurmSchedLogFile=
#SlurmSchedLogLevel=
#
#
# POWER SAVE SUPPORT FOR IDLE NODES (optional)
#SuspendProgram=
#ResumeProgram=
#SuspendTimeout=
#ResumeTimeout=
#ResumeRate=
#SuspendExcNodes=
#SuspendExcParts=
#SuspendRate=
#SuspendTime=
#
#
# COMPUTE NODES
#
GresTypes=gpu
NodeName=dynomics NodeAddr=10.0.3.1 CPUs=256 CoresPerSocket=64 SocketsPerBoard=2 ThreadsPerCore=2 State=UNKNOWN Gres=gpu:7g.40gb:2,gpu:1g.5gb:28,gpu:2g.10gb:4,gpu:3g.20gb:2 RealMemory=900000
PartitionName=COMPUTE1Q Nodes=dynomics Default=YES Priority=1 DefMemPerNode=16000 MaxCPUsPerNode=248 TRESBillingWeights="CPU=1.0,Mem=0.25G,GRES/gpu=42,GRES/gpu:7g.40gb=42,GRES/gpu:1g.5gb=6,GRES/gpu:2g.10gb=12,GRES/gpu:3g.20gb=18" MaxTime=4-0 State=UP 
PartitionName=ADMINQ Nodes=dynomics Default=NO Priority=65533 DefMemPerNode=16000 MaxTime=INFINITE State=UP AllowAccounts=root
